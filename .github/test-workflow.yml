---
name: Tests - Integration - AWS OpenShift ROSA HCP Single Region

on:
    schedule:
        - cron: 0 3 * * 1 # Runs at 3 AM on Monday
    pull_request:
        paths:
            - .github/workflows/aws_openshift_rosa_hcp_single_region_tests.yml
            - .github/workflows-config/aws-openshift-rosa-hcp-single-region/test_matrix.yml
            - .tool-versions
            - generic/kubernetes/single-region/**
            - generic/openshift/single-region/**
            - aws/openshift/rosa-hcp-single-region/**
            - '!aws/openshift/rosa-hcp-single-region/test/golden/**'
            - .github/actions/aws-openshift-rosa-hcp-single-region-create/**
            - .github/actions/aws-openshift-rosa-hcp-single-region-cleanup/**

    workflow_dispatch:
        inputs:
            cluster_name:
                description: Cluster name.
                required: false
                type: string
            delete_clusters:
                description: Whether to delete the clusters.
                type: boolean
                default: true
            enable_tests:
                description: Whether to enable the tests.
                type: boolean
                default: true

# limit to a single execution per actor of this workflow
concurrency:
    group: ${{ github.workflow }}-${{ github.ref }}
    # in case of renovate we don't cancel the previous run, so it can finish it
    # otherwise weekly renovate PRs with tf docs updates result in broken clusters
    cancel-in-progress: ${{ !contains('renovate[bot]', github.actor) }}

env:
    IS_SCHEDULE: ${{ contains(github.ref, 'refs/heads/schedules/') || github.event_name == 'schedule' && 'true' || 'false' }}

    AWS_PROFILE: infex
    AWS_REGION: eu-west-2
    S3_BACKEND_BUCKET: tests-ra-aws-rosa-hcp-tf-state-eu-central-1
    S3_BUCKET_REGION: eu-central-1

    CLEANUP_CLUSTERS: ${{ github.event.inputs.delete_clusters || 'true' }}

    # TEST VARIABLES

    # Vars with "CI_" prefix are used in the CI workflow only.
    CI_MATRIX_FILE: .github/workflows-config/aws-openshift-rosa-hcp-single-region/test_matrix.yml

    # Docker Hub auth to avoid image pull rate limit.
    # Vars with "TEST_" prefix are used in the test runner tool (Task).
    TESTS_ENABLED: ${{ github.event.inputs.enable_tests || 'true' }}
    # renovate: datasource=github-tags depName=camunda/camunda-platform-helm
    TESTS_CAMUNDA_HELM_CHART_REPO_REF: 11.2.2   # git reference used to clone the camunda/camunda-platform-helm repository to perform the tests
    TESTS_CAMUNDA_HELM_CHART_REPO_PATH: ./.camunda_helm_repo   # where to clone it

    # Components that are not enabled by default in the doc, but enabled in our tests to have a better coverage
    WEBMODELER_ENABLED: 'true'
    CONSOLE_ENABLED: 'true'

    ROSA_CLI_VERSION: latest

jobs:
    triage:
        runs-on: ubuntu-latest
        outputs:
            should_skip: ${{ steps.skip_check.outputs.should_skip }}
        steps:
            - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4
            - name: Check labels
              id: skip_check
              uses: ./.github/actions/internal-triage-skip

    clusters-info:
        needs:
            - triage
        if: needs.triage.outputs.should_skip == 'false'
        name: Define Matrix
        runs-on: ubuntu-latest
        outputs:
            platform-matrix: ${{ steps.matrix.outputs.platform-matrix }}
        steps:
            - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4
              with:
                  fetch-depth: 0

            - name: Install asdf tools with cache
              uses: camunda/infraex-common-config/./.github/actions/asdf-install-tooling@6dc218bf7ee3812a4b6b13c305bce60d5d1d46e5 # 1.3.1

            - id: matrix
              # we define a global matrix in an external file due to https://github.com/orgs/community/discussions/26284
              run: |
                  set -euxo pipefail # tolerate, nothing.

                  # Generate cluster name.
                  # shellcheck disable=SC2086
                  distro_indexes="$(yq '.matrix.distro | to_entries | .[] | .key' ${CI_MATRIX_FILE})"

                  # Loop over clusters.
                  # Vars are exported to pass them to yq instead of local inline syntax.
                  # shellcheck disable=SC2086
                  for distro_index in ${distro_indexes}; do
                    cluster_name_input="${{ inputs.cluster_name }}"
                    cluster_name_fallback="hci-$(uuidgen | head -c 8)"
                    export cluster_name="${cluster_name_input:-${cluster_name_fallback}}"
                    export distro_index="${distro_index}"
                    yq -i '.matrix.distro[env(distro_index)].clusterName = env(cluster_name)' "${CI_MATRIX_FILE}"
                  done

                  echo "Filtering the matrix with strategy IS_SCHEDULE=$IS_SCHEDULE"
                  if [[ "$IS_SCHEDULE" == "true" ]]; then
                    # shellcheck disable=SC2086
                    platform_matrix="$(yq '.matrix |= (.distro |= map(select(.schedule_only == true)))' \
                      --indent=0 --output-format json ${CI_MATRIX_FILE})"
                  else
                    # shellcheck disable=SC2086
                    platform_matrix="$(yq '.matrix |= (.distro |= map(select(.schedule_only == null or .schedule_only == false)))' \
                      --indent=0 --output-format json ${CI_MATRIX_FILE})"
                  fi

                  platform_matrix="$(echo "$platform_matrix" | yq '.matrix' --indent=0 --output-format json)"
                  echo "${platform_matrix}" | jq
                  echo "platform-matrix=${platform_matrix}" > "$GITHUB_OUTPUT"

    prepare-clusters:
        name: Prepare clusters
        needs:
            - clusters-info
        strategy:
            fail-fast: false
            matrix:
                distro: ${{ fromJson(needs.clusters-info.outputs.platform-matrix).distro }}
        runs-on: ubuntu-latest
        steps:
            - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4
              with:
                  ref: ${{ github.ref }}
                  fetch-depth: 0

            - name: Install asdf tools with cache
              uses: camunda/infraex-common-config/./.github/actions/asdf-install-tooling@6dc218bf7ee3812a4b6b13c305bce60d5d1d46e5 # 1.3.1

            - name: Import Secrets
              id: secrets
              uses: hashicorp/vault-action@7709c609789c5e27b757a85817483caadbb5939a # v3
              with:
                  url: ${{ secrets.VAULT_ADDR }}
                  method: approle
                  roleId: ${{ secrets.VAULT_ROLE_ID }}
                  secretId: ${{ secrets.VAULT_SECRET_ID }}
                  exportEnv: false
                  secrets: |
                      secret/data/products/infrastructure-experience/ci/common AWS_ACCESS_KEY;
                      secret/data/products/infrastructure-experience/ci/common AWS_SECRET_KEY;
                      secret/data/products/infrastructure-experience/ci/common RH_OPENSHIFT_TOKEN;
                      secret/data/products/infrastructure-experience/ci/common CI_OPENSHIFT_MAIN_PASSWORD;
                      secret/data/products/infrastructure-experience/ci/common CI_OPENSHIFT_MAIN_USERNAME;

            - name: Add profile credentials to ~/.aws/credentials
              shell: bash
              run: |
                  aws configure set aws_access_key_id ${{ steps.secrets.outputs.AWS_ACCESS_KEY }} --profile ${{ env.AWS_PROFILE }}
                  aws configure set aws_secret_access_key ${{ steps.secrets.outputs.AWS_SECRET_KEY }} --profile ${{ env.AWS_PROFILE }}
                  aws configure set region ${{ env.AWS_REGION }} --profile ${{ env.AWS_PROFILE }}

            - name: Set current Camunda version
              id: camunda-version
              run: |
                  CAMUNDA_VERSION=$(cat .camunda-version)
                  echo "CAMUNDA_VERSION=$CAMUNDA_VERSION" | tee -a "$GITHUB_OUTPUT"

            # Also remove the versioning
            - name: Create ROSA cluster and login
              uses: ./.github/actions/aws-openshift-rosa-hcp-single-region-create
              id: create_cluster
              # Do not interrupt tests; otherwise, the Terraform state may become inconsistent.
              if: always() && success()
              with:
                  rh-token: ${{ steps.secrets.outputs.RH_OPENSHIFT_TOKEN }}
                  cluster-name: ${{ matrix.distro.clusterName }}
                  admin-username: ${{ steps.secrets.outputs.CI_OPENSHIFT_MAIN_USERNAME }}
                  admin-password: ${{ steps.secrets.outputs.CI_OPENSHIFT_MAIN_PASSWORD }}
                  aws-region: ${{ env.AWS_REGION }}
                  s3-backend-bucket: ${{ env.S3_BACKEND_BUCKET }}
                  s3-bucket-region: ${{ env.S3_BUCKET_REGION }}
                  s3-bucket-key-prefix: ${{ steps.camunda-version.outputs.CAMUNDA_VERSION }}/
                  openshift-version: ${{ matrix.distro.version }}
                  tf-modules-revision: ${{ github.ref }}

            - name: Export kubeconfig and encrypt it # this is required to pass matrix outputs securely using artifacts
              id: export_kube_config
              run: |
                  # shellcheck disable=SC2005
                  echo "$(kubectl config view --raw)" > kubeconfig.yaml 2>/dev/null
                  openssl enc -aes-256-cbc -salt -in kubeconfig.yaml -out encrypted_kubeconfig.enc -pass pass:"${GITHUB_TOKEN}" -pbkdf2
                  encrypted_kubeconfig_base64=$(base64 -w 0 encrypted_kubeconfig.enc)
                  echo "kubeconfig_raw=${encrypted_kubeconfig_base64}" >> "$GITHUB_OUTPUT"

            ## Write for matrix outputs workaround
            - uses: cloudposse/github-action-matrix-outputs-write@ed06cf3a6bf23b8dce36d1cf0d63123885bb8375 # v1
              id: out
              with:
                  matrix-step-name: ${{ github.job }}
                  matrix-key: ${{ matrix.distro.name }}
                  outputs: |-
                      kubeconfig_raw: ${{ steps.export_kube_config.outputs.kubeconfig_raw }}

    access-info:
        name: Read kube configs from matrix
        runs-on: ubuntu-latest
        needs: prepare-clusters
        outputs:
            kubeconfig: ${{ steps.read-workflow.outputs.result }}
        steps:
            - uses: cloudposse/github-action-matrix-outputs-read@33cac12fa9282a7230a418d859b93fdbc4f27b5a # v1
              id: read-workflow
              with:
                  matrix-step-name: prepare-clusters

    integration-tests:
        name: Run integration tests - ${{ matrix.distro.name }}
        runs-on: ubuntu-latest
        needs:
            - clusters-info
            - access-info
        strategy:
            fail-fast: false
            matrix:
                distro: ${{ fromJson(needs.clusters-info.outputs.platform-matrix).distro }}
                scenario: ${{ fromJson(needs.clusters-info.outputs.platform-matrix).scenario }}
        env:
            TEST_NAMESPACE: camunda   # This namespace is hard-coded in the documentation
            # https://github.com/camunda/camunda-platform-helm/blob/test/integration/scenarios/chart-full-setup/Taskfile.yaml#L12C15-L12C32
            TEST_CLUSTER_TYPE: openshift
        steps:
            - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4
              if: env.TESTS_ENABLED == 'true'
              timeout-minutes: 20
              run: |
                  set -euxo pipefail # tolerate, nothing.

                  echo "Show zeebe cluster topology using generic/kubernetes/single-region/procedure/check-zeebe-cluster-topology.sh:"
                  export ZEEBE_CLIENT_ID="${{ steps.secrets.outputs.CI_CAMUNDA_USER_TEST_CLIENT_ID }}"
                  export ZEEBE_CLIENT_SECRET="${{ steps.secrets.outputs.CI_CAMUNDA_USER_TEST_CLIENT_SECRET }}"
                  export DOMAIN_NAME="$CAMUNDA_DOMAIN"

                  # Execute the script and capture the output in a variable
                  check_zeebe_topology_output=$(./generic/kubernetes/single-region/procedure/check-zeebe-cluster-topology.sh)

                  echo "$check_zeebe_topology_output" | jq

                  # Checks
                  error_found=false
                  check_zeebe_topology_all_healthy=$(echo "$check_zeebe_topology_output" | jq '[.brokers[].partitions[].health == "healthy"] | all')
                  check_zeebe_topology_cluster_size=$(echo "$check_zeebe_topology_output" | jq '.clusterSize')
                  check_zeebe_topology_partitions_count=$(echo "$check_zeebe_topology_output" | jq '.partitionsCount')

                  if [ "$check_zeebe_topology_all_healthy" = "true" ]; then
                    echo "✅ All partitions are healthy."
                  else
                    echo "❌ Not all partitions are healthy"
                    error_found=true
                  fi

                  if [ "$check_zeebe_topology_cluster_size" -eq 3 ]; then
                    echo "✅ Cluster size is 3."
                  else
                    echo "❌ Cluster size is not 3."
                    error_found=true
                  fi

                  if [ "$check_zeebe_topology_partitions_count" -eq 3 ]; then
                    echo "✅ Partitions count is 3."
                  else
                    echo "❌ Partitions count is not 3."
                    error_found=true
                  fi

                  echo "Comparing golden file of the zeebe topology output..."

                  reference_file="./generic/kubernetes/single-region/procedure/check-zeebe-cluster-topology-output.json"
                  # Save the output to a temporary file
                  temp_output=$(mktemp)
                  echo "$check_zeebe_topology_output" > "$temp_output"

                  # Replace patch version
                  yq e '.brokers[].version |= sub("[.].*$", ".z") | .gatewayVersion |= sub("[.].*$", ".z")' -i "$temp_output"
                  yq e '.brokers[].version |= sub("[.].*$", ".z") | .gatewayVersion |= sub("[.].*$", ".z")' -i "$reference_file"

                  # Order each file also remove not predictable fields
                  yq e '.brokers |= sort_by(.host) | .brokers[] |= (.partitions |= sort_by(.partitionId) | .partitions[].role = "NOT_PREDICTABLE")' -i "$temp_output"
                  yq e '.brokers |= sort_by(.host) | .brokers[] |= (.partitions |= sort_by(.partitionId) | .partitions[].role = "NOT_PREDICTABLE")' -i "$reference_file"

                  # Compare the two files using diff (in compacted JSON format)
                  diff_output=$(delta <(jq -S . "$temp_output") <(jq -S . "$reference_file"))

                  if [[ -n "$diff_output" ]]; then
                    # If differences are found, print the error and the diff
                    echo "❌ Error: The golden files of zeebe topology files do not match."
                    echo "Differences found:"
                    echo "$diff_output"

                    # Display the new generated version
                    echo "New version:"
                    cat "$temp_output"

                    error_found=true
                  fi

                  if [ "$error_found" = true ]; then
                    echo "❌ Some tests failed."
                    exit 1
                  fi
                  echo "✅ The cluster meets all the expected criteria."

            - name: 🔬🚨 Get failed Pods info
              if: failure()
              uses: camunda/camunda-platform-helm/./.github/actions/failed-pods-info@50b0d04068988813cebe3909a2211d8e063b6499 # camunda-platform-11.2.2
